{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nfrom xgboost import XGBRegressor\n\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:49:45.294933Z","iopub.execute_input":"2021-10-02T08:49:45.295275Z","iopub.status.idle":"2021-10-02T08:49:45.300059Z","shell.execute_reply.started":"2021-10-02T08:49:45.295243Z","shell.execute_reply":"2021-10-02T08:49:45.298972Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"random_state = 42\nKF_split = 7\nKF_split_optuna = 5\noptuna_trials = 15\n\noptuna.logging.set_verbosity(optuna.logging.WARNING) #stop showing each trial result","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:49:45.628655Z","iopub.execute_input":"2021-10-02T08:49:45.628990Z","iopub.status.idle":"2021-10-02T08:49:45.633594Z","shell.execute_reply.started":"2021-10-02T08:49:45.628960Z","shell.execute_reply":"2021-10-02T08:49:45.632621Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\n# Creating a DataFrame for Blending\ny_valid_pred_blnd = pd.DataFrame(data=df_train.target,index=df_train.index).reset_index()\ny_test_pred_blnd = pd.DataFrame(index=df_test.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:49:45.989730Z","iopub.execute_input":"2021-10-02T08:49:45.990073Z","iopub.status.idle":"2021-10-02T08:49:47.718137Z","shell.execute_reply.started":"2021-10-02T08:49:45.990040Z","shell.execute_reply":"2021-10-02T08:49:47.717238Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:35:21.583724Z","iopub.execute_input":"2021-10-02T08:35:21.584044Z","iopub.status.idle":"2021-10-02T08:35:21.616821Z","shell.execute_reply.started":"2021-10-02T08:35:21.584008Z","shell.execute_reply":"2021-10-02T08:35:21.615854Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optuna_tuning_fitting(model_name):\n    \n    # Optimizing Using Optuna\n    def objective(trial):\n        \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n\n        \n        rmse=0\n\n        kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n        for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n        \n            # Generating X and y for train and test sets\n            X_train_f = X_train.iloc[train_idx].copy()\n            y_train_f = y_train.iloc[train_idx]\n\n            X_valid_f = X_train.iloc[valid_idx].copy()\n            y_valid_f = y_train.iloc[valid_idx]\n\n            X_test = df_test.copy()\n\n\n            if model_name == 'model_1':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n                # Scaling Features\n                scaler = preprocessing.StandardScaler()\n                X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n                X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n                X_test[num_col] = scaler.transform(X_test[num_col])\n\n\n            elif model_name == 'model_2':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n                # Scaling Features\n                scaler = preprocessing.StandardScaler()\n                X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n                X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n                X_test[num_col] = scaler.transform(X_test[num_col])\n            \n\n            elif model_name == 'model_3':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n\n            elif model_name == 'model_4':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                X_test[cat_col] = encoder.transform(X_test[cat_col])    \n            \n\n            elif model_name == 'model_5':\n                df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n\n                for col in cat_col:\n                    map_dict = df_train_f.groupby(col).mean().target.to_dict()\n                    X_train_f[col] = X_train_f[col].map(map_dict)\n                    X_valid_f[col] = X_valid_f[col].map(map_dict)\n                    X_test[col] = X_test[col].map(map_dict)\n\n            \n\n            # Modeling \n            model = XGBRegressor(**params,\n                                random_state=fold,\n                                tree_method='gpu_hist',\n                                gpu_id=0,\n                                predictor='gpu_predictor')\n        \n            model.fit(X_train_f, y_train_f,\n                    eval_set=[(X_valid_f,y_valid_f)],\n                    early_stopping_rounds=300,\n                    verbose=False)\n            \n            y_pred_f = model.predict(X_valid_f)\n            rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n            return rmse\n        \n        \n    # Optimizing\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=optuna_trials)\n\n    print(f'Best score: {study.best_value:.5f}')\n    print(f'Best Params: {study.best_params}')\n\n#-----------------------------------------------------------------------------------------------\n\n    # Fitting the tuned Model\n    y_test_pred = []\n\n    kf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n        \n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n        \n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n        \n        X_test = df_test.copy()\n\n\n\n        if model_name == 'model_1':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n            # Scaling Features\n            scaler = preprocessing.StandardScaler()\n            X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n            X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n            X_test[num_col] = scaler.transform(X_test[num_col])\n\n\n        elif model_name == 'model_2':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n            # Scaling Features\n            scaler = preprocessing.StandardScaler()\n            X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n            X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n            X_test[num_col] = scaler.transform(X_test[num_col])\n\n\n        elif model_name == 'model_3':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n\n        elif model_name == 'model_4':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])    \n\n\n        elif model_name == 'model_5':\n            df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n\n            for col in cat_col:\n                map_dict = df_train_f.groupby(col).mean().target.to_dict()\n                X_train_f[col] = X_train_f[col].map(map_dict)\n                X_valid_f[col] = X_valid_f[col].map(map_dict)\n                X_test[col] = X_test[col].map(map_dict)\n\n        \n\n        # Modeling\n        model = XGBRegressor(**study.best_params,\n                            random_state=fold,\n                            tree_method='gpu_hist',\n                            gpu_id=0,\n                            predictor='gpu_predictor')\n        \n        model.fit(X_train_f, y_train_f,\n                    eval_set=[(X_valid_f,y_valid_f)],\n                    early_stopping_rounds=300,\n                    verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        print(f'fold-{fold} rmse : {rmse:.5f}')\n        \n        y_test_f = model.predict(X_test)\n        y_test_pred.append(y_test_f)\n        \n        # Updating Blending DataFrame\n        y_valid_pred_blnd.loc[valid_idx, model_name] = y_pred_f\n        \n    y_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\n    y_test_pred_blnd.loc[:, model_name] = y_test_model\n\n\n    #return y_valid_pred_blnd, y_test_pred_blnd\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:35:21.618430Z","iopub.execute_input":"2021-10-02T08:35:21.618773Z","iopub.status.idle":"2021-10-02T08:35:21.654664Z","shell.execute_reply.started":"2021-10-02T08:35:21.618734Z","shell.execute_reply":"2021-10-02T08:35:21.653775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\noptuna_tuning_fitting(model_name='model_1')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:35:21.655773Z","iopub.execute_input":"2021-10-02T08:35:21.656985Z","iopub.status.idle":"2021-10-02T08:38:01.346854Z","shell.execute_reply.started":"2021-10-02T08:35:21.656946Z","shell.execute_reply":"2021-10-02T08:38:01.345824Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\noptuna_tuning_fitting(model_name='model_2')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:38:01.348395Z","iopub.execute_input":"2021-10-02T08:38:01.348930Z","iopub.status.idle":"2021-10-02T08:40:44.578860Z","shell.execute_reply.started":"2021-10-02T08:38:01.348890Z","shell.execute_reply":"2021-10-02T08:40:44.577905Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nnum_col = [col for col in df_train.columns if 'cont' in col]\ncat_col = [col for col in df_train.columns if 'cat' in col]\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\n# Log tranformation\nfor col in num_col:\n    X_train[col] = np.log1p(X_train[col])\n    X_test[col] = np.log1p(X_test[col])\n    \noptuna_tuning_fitting(model_name='model_3')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:40:44.581671Z","iopub.execute_input":"2021-10-02T08:40:44.581976Z","iopub.status.idle":"2021-10-02T08:43:18.321441Z","shell.execute_reply.started":"2021-10-02T08:40:44.581942Z","shell.execute_reply":"2021-10-02T08:43:18.320492Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1).copy()\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\n# Polynomials\npoly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n\ntrain_poly = poly.fit_transform(X_train[num_col])\nX_train_poly = pd.DataFrame(train_poly,\n                            columns=[f'poly_{i}' for i in range(train_poly.shape[1])],\n                            index=X_train.index) # using index for cancatenation\nX_train = pd.concat([X_train[cat_col],X_train_poly], axis=1) # To avoid duplicating, we just concat cat_col with poly dataframe\n\ntest_poly = poly.fit_transform(df_test[num_col])\ndf_test_poly = pd.DataFrame(test_poly,\n                            columns=[f'poly_{i}' for i in range(test_poly.shape[1])],\n                            index=X_test.index)\ndf_test = pd.concat([X_test[cat_col],df_test_poly], axis=1)\n\noptuna_tuning_fitting(model_name='model_4')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:43:18.323357Z","iopub.execute_input":"2021-10-02T08:43:18.323861Z","iopub.status.idle":"2021-10-02T08:47:13.950291Z","shell.execute_reply.started":"2021-10-02T08:43:18.323821Z","shell.execute_reply":"2021-10-02T08:47:13.949359Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1).copy()\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\noptuna_tuning_fitting(model_name='model_5')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:47:13.951925Z","iopub.execute_input":"2021-10-02T08:47:13.952282Z","iopub.status.idle":"2021-10-02T08:48:50.673882Z","shell.execute_reply.started":"2021-10-02T08:47:13.952245Z","shell.execute_reply":"2021-10-02T08:48:50.672951Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"y_valid_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.675227Z","iopub.execute_input":"2021-10-02T08:48:50.675714Z","iopub.status.idle":"2021-10-02T08:48:50.694861Z","shell.execute_reply.started":"2021-10-02T08:48:50.675675Z","shell.execute_reply":"2021-10-02T08:48:50.694094Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_valid_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.696062Z","iopub.execute_input":"2021-10-02T08:48:50.696432Z","iopub.status.idle":"2021-10-02T08:48:50.714022Z","shell.execute_reply.started":"2021-10-02T08:48:50.696398Z","shell.execute_reply":"2021-10-02T08:48:50.713060Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"y_test_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.715490Z","iopub.execute_input":"2021-10-02T08:48:50.715914Z","iopub.status.idle":"2021-10-02T08:48:50.732644Z","shell.execute_reply.started":"2021-10-02T08:48:50.715879Z","shell.execute_reply":"2021-10-02T08:48:50.731616Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y_test_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.734145Z","iopub.execute_input":"2021-10-02T08:48:50.734590Z","iopub.status.idle":"2021-10-02T08:48:50.752407Z","shell.execute_reply.started":"2021-10-02T08:48:50.734554Z","shell.execute_reply":"2021-10-02T08:48:50.751502Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"a=y_valid_pred_blnd\nb=y_test_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.753747Z","iopub.execute_input":"2021-10-02T08:48:50.754080Z","iopub.status.idle":"2021-10-02T08:48:50.760099Z","shell.execute_reply.started":"2021-10-02T08:48:50.754047Z","shell.execute_reply":"2021-10-02T08:48:50.759252Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df sd fsd fsd fsdf ","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.761380Z","iopub.execute_input":"2021-10-02T08:48:50.761845Z","iopub.status.idle":"2021-10-02T08:48:50.770516Z","shell.execute_reply.started":"2021-10-02T08:48:50.761797Z","shell.execute_reply":"2021-10-02T08:48:50.767815Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model #1 ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:49:58.441733Z","iopub.execute_input":"2021-10-02T08:49:58.442057Z","iopub.status.idle":"2021-10-02T08:50:00.218288Z","shell.execute_reply.started":"2021-10-02T08:49:58.442026Z","shell.execute_reply":"2021-10-02T08:50:00.217394Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n        # Encoding Categorical variables\n        encoder = preprocessing.OrdinalEncoder()\n        X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n        X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n        X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n        # Scaling Features\n        scaler = preprocessing.StandardScaler()\n        X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n        X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n        X_test[num_col] = scaler.transform(X_test[num_col])\n\n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:50:00.219844Z","iopub.execute_input":"2021-10-02T08:50:00.220199Z","iopub.status.idle":"2021-10-02T08:51:48.786174Z","shell.execute_reply.started":"2021-10-02T08:50:00.220161Z","shell.execute_reply":"2021-10-02T08:51:48.785229Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n    \n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n    \n    X_test = df_test.copy()\n    \n    # Encoding Categorical variables\n    encoder = preprocessing.OrdinalEncoder()\n    X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n    X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n    X_test[cat_col] = encoder.transform(X_test[cat_col])\n    \n    # Scaling Features\n    scaler = preprocessing.StandardScaler()\n    X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n    X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n    X_test[num_col] = scaler.transform(X_test[num_col])\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx,'model_1'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:,'model_1'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:51:48.787856Z","iopub.execute_input":"2021-10-02T08:51:48.788365Z","iopub.status.idle":"2021-10-02T08:52:38.320507Z","shell.execute_reply.started":"2021-10-02T08:51:48.788318Z","shell.execute_reply":"2021-10-02T08:52:38.319618Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Model #2","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:52:38.322052Z","iopub.execute_input":"2021-10-02T08:52:38.322584Z","iopub.status.idle":"2021-10-02T08:52:40.083372Z","shell.execute_reply.started":"2021-10-02T08:52:38.322545Z","shell.execute_reply":"2021-10-02T08:52:40.082476Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n        # Encoding Categorical variables\n        encoder = preprocessing.OrdinalEncoder()\n        X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n        X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n        X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n        # Scaling Features\n        scaler = preprocessing.StandardScaler()\n        X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n        X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n        X_test[num_col] = scaler.transform(X_test[num_col])\n\n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:52:40.084827Z","iopub.execute_input":"2021-10-02T08:52:40.085234Z","iopub.status.idle":"2021-10-02T08:54:28.215105Z","shell.execute_reply.started":"2021-10-02T08:52:40.085199Z","shell.execute_reply":"2021-10-02T08:54:28.213546Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n    \n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n    \n    X_test = df_test.copy()\n    \n    # Encoding Categorical variables\n    encoder = preprocessing.OrdinalEncoder()\n    X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n    X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n    X_test[cat_col] = encoder.transform(X_test[cat_col])\n    \n    # Scaling Features\n    scaler = preprocessing.StandardScaler()\n    X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n    X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n    X_test[num_col] = scaler.transform(X_test[num_col])\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx,'model_2'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:,'model_2'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:54:28.216564Z","iopub.execute_input":"2021-10-02T08:54:28.216915Z","iopub.status.idle":"2021-10-02T08:55:17.896065Z","shell.execute_reply.started":"2021-10-02T08:54:28.216877Z","shell.execute_reply":"2021-10-02T08:55:17.895134Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Model #3","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nnum_col = [col for col in df_train.columns if 'cont' in col]\ncat_col = [col for col in df_train.columns if 'cat' in col]\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\n# Log tranformation\nfor col in num_col:\n    X_train[col] = np.log1p(X_train[col])\n    X_test[col] = np.log1p(X_test[col])","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:55:17.897342Z","iopub.execute_input":"2021-10-02T08:55:17.897847Z","iopub.status.idle":"2021-10-02T08:55:19.754917Z","shell.execute_reply.started":"2021-10-02T08:55:17.897806Z","shell.execute_reply":"2021-10-02T08:55:19.754065Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n        # Encoding Categorical variables\n        encoder = preprocessing.OrdinalEncoder()\n        X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n        X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n        X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:55:19.757288Z","iopub.execute_input":"2021-10-02T08:55:19.757635Z","iopub.status.idle":"2021-10-02T08:57:07.721522Z","shell.execute_reply.started":"2021-10-02T08:55:19.757599Z","shell.execute_reply":"2021-10-02T08:57:07.720497Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n    \n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n    \n    X_test = df_test.copy()\n    \n    # Encoding Categorical variables\n    encoder = preprocessing.OrdinalEncoder()\n    X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n    X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n    X_test[cat_col] = encoder.transform(X_test[cat_col])\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx,'model_3'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:,'model_3'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:57:07.723394Z","iopub.execute_input":"2021-10-02T08:57:07.723912Z","iopub.status.idle":"2021-10-02T08:57:55.587173Z","shell.execute_reply.started":"2021-10-02T08:57:07.723872Z","shell.execute_reply":"2021-10-02T08:57:55.586262Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Model #4","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1).copy()\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\n# Polynomials\npoly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n\ntrain_poly = poly.fit_transform(X_train[num_col])\nX_train_poly = pd.DataFrame(train_poly,\n                            columns=[f'poly_{i}' for i in range(train_poly.shape[1])],\n                            index=X_train.index) # using index for cancatenation\nX_train = pd.concat([X_train[cat_col],X_train_poly], axis=1) # To avoid duplicating, we just concat cat_col with poly dataframe\n\ntest_poly = poly.fit_transform(df_test[num_col])\ndf_test_poly = pd.DataFrame(test_poly,\n                            columns=[f'poly_{i}' for i in range(test_poly.shape[1])],\n                            index=X_test.index)\ndf_test = pd.concat([X_test[cat_col],df_test_poly], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:57:55.588456Z","iopub.execute_input":"2021-10-02T08:57:55.588955Z","iopub.status.idle":"2021-10-02T08:57:58.602473Z","shell.execute_reply.started":"2021-10-02T08:57:55.588918Z","shell.execute_reply":"2021-10-02T08:57:58.601447Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n        # Encoding Categorical variables\n        encoder = preprocessing.OrdinalEncoder()\n        X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n        X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n        X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:57:58.603837Z","iopub.execute_input":"2021-10-02T08:57:58.604233Z","iopub.status.idle":"2021-10-02T09:00:37.978393Z","shell.execute_reply.started":"2021-10-02T08:57:58.604194Z","shell.execute_reply":"2021-10-02T09:00:37.976863Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n    \n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n    \n    X_test = df_test.copy()\n    \n    # Encoding Categorical variables\n    encoder = preprocessing.OrdinalEncoder()\n    X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n    X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n    X_test[cat_col] = encoder.transform(X_test[cat_col])\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx,'model_4'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:,'model_4'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T09:00:37.979694Z","iopub.execute_input":"2021-10-02T09:00:37.980052Z","iopub.status.idle":"2021-10-02T09:01:51.705242Z","shell.execute_reply.started":"2021-10-02T09:00:37.980014Z","shell.execute_reply":"2021-10-02T09:01:51.704388Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Model #5","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/30-days-of-ml/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]","metadata":{"execution":{"iopub.status.busy":"2021-10-02T09:01:51.706505Z","iopub.execute_input":"2021-10-02T09:01:51.706851Z","iopub.status.idle":"2021-10-02T09:01:53.447441Z","shell.execute_reply.started":"2021-10-02T09:01:51.706818Z","shell.execute_reply":"2021-10-02T09:01:53.446585Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n#         params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n#             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 7),\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n#         }\n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n            'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n        }\n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n        # Generating X and y for train train_idxd test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n        df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n\n        for col in cat_col:\n            map_dict = df_train_f.groupby(col).mean().target.to_dict()\n            X_train_f[col] = X_train_f[col].map(map_dict)\n            X_valid_f[col] = X_valid_f[col].map(map_dict)\n            X_test[col] = X_test[col].map(map_dict)\n\n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T09:01:53.448792Z","iopub.execute_input":"2021-10-02T09:01:53.449131Z","iopub.status.idle":"2021-10-02T09:03:00.062974Z","shell.execute_reply.started":"2021-10-02T09:01:53.449095Z","shell.execute_reply":"2021-10-02T09:03:00.062052Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n    # Generating X and y for train train_idxd test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n    \n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n    \n    X_test = df_test.copy()\n    \n    df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n    \n    for col in cat_col:\n        map_dict = df_train_f.groupby(col).mean().target.to_dict()\n        X_train_f[col] = X_train_f[col].map(map_dict)\n        X_valid_f[col] = X_valid_f[col].map(map_dict)\n        X_test[col] = X_test[col].map(map_dict)\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx,'model_5'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:,'model_5'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T09:03:00.064251Z","iopub.execute_input":"2021-10-02T09:03:00.064762Z","iopub.status.idle":"2021-10-02T09:03:30.734678Z","shell.execute_reply.started":"2021-10-02T09:03:00.064721Z","shell.execute_reply":"2021-10-02T09:03:30.733680Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"markdown","source":"## Blending #1","metadata":{}},{"cell_type":"code","source":"X_train_blnd = y_valid_pred_blnd.drop(['id','target'], axis=1)\ny_train_blnd = y_valid_pred_blnd.target\n\nX_test_blnd = y_test_pred_blnd","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.802036Z","iopub.status.idle":"2021-10-02T08:48:50.802613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n        'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n    }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n\n        X_train_f = X_train_blnd.iloc[train_idx].copy()\n        y_train_f = y_train_blnd.iloc[train_idx]\n\n        X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n        y_valid_f = y_train_blnd.iloc[valid_idx]\n\n        X_test = X_test_blnd.copy()\n    \n    \n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\n# optuna.logging.set_verbosity(optuna.logging.WARNING) #stop showing each trial result\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.803934Z","iopub.status.idle":"2021-10-02T08:48:50.804554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train_blnd.iloc[train_idx].copy()\n    y_train_f = y_train_blnd.iloc[train_idx]\n    \n    X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n    y_valid_f = y_train_blnd.iloc[valid_idx]\n    \n    X_test = X_test_blnd.copy()\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n#     # Updating Blending DataFrame\n#     y_valid_pred_blnd.loc[valid_idx,'model_1'] = y_pred_f\n    \n# y_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\n# y_test_pred_blnd.loc[:,'model_1'] = y_test_model","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.806018Z","iopub.status.idle":"2021-10-02T08:48:50.806601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dr r  r ","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.807891Z","iopub.status.idle":"2021-10-02T08:48:50.808473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n    \n    X_train_f = X_train_blnd.iloc[train_idx].copy()\n    y_train_f = y_train_blnd.iloc[train_idx]\n    \n    X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n    y_valid_f = y_train_blnd.iloc[valid_idx]\n    \n    X_test = X_test_blnd.copy()\n    \n    # Modeling\n    model = XGBRegressor(random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T08:48:50.809819Z","iopub.status.idle":"2021-10-02T08:48:50.810393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}